<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>GMM</title>
    <url>/GMM.html</url>
    <content><![CDATA[<p>高斯混合模型学习</p>
<a id="more"></a>

<h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>参数概率密度函数，用gaussian component densities的加权和表示。</p>
<p>GMM参数估计是从训练数据使用迭代期望最大化（EM）算法或最大后验（MAP）估计从一个训练有素的先验模型。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>$$<br>p(\text{x}|\lambda)=\sum_{i=1}^M\omega_ig(\text{x}|\mu_i,\begin{matrix}\sum_i\end{matrix})<br>$$</p>
<p>where<br>$$<br>g(\text{x}|\mu_i,\begin{matrix}\sum_i\end{matrix})=\frac{1}{(2\pi)^{D/2}|\begin{matrix}\sum_i\end{matrix}|^{1/2}}\text{exp} { -\frac{1}{2}(\text{x}-\mu_i)’\begin{matrix}\sum_i^{-1}\end{matrix}(\text{x}-\mu_i)}<br>$$<br>with mean vextor $\mu_i$ and covariance matrix $\sum_i$, the weight satisfy the constraint that $\sum_{i=1}^M\omega_i=1$.</p>
<p>in the formular, the parameters can be denoted by<br>$$<br>\lambda={ \omega_i,\mu_i,\begin{matrix}\sum_i\end{matrix}}\qquad i=1,2,…,M<br>$$<br>then, </p>
<img src="/GMM/a1.png" class="" title="This is a picture">

<blockquote>
<p>Fig. 1. Comparison of distribution modeling. (a) histogram of a single cepstral coefficient from a 25 second utterance by a male speaker (b)maximum likelihood uni-modal Gaussian model (c) GMM and its 10 underlying component densities (d) histogram of the data assigned to the VQ centroid locations of a 10 element codebook.</p>
</blockquote>
]]></content>
      <categories>
        <category>传统方法</category>
        <category>理论学习</category>
      </categories>
      <tags>
        <tag>传统方法</tag>
        <tag>GMM</tag>
      </tags>
  </entry>
  <entry>
    <title>Needles in a Haystack Tracking City-Scale Moving Vehicles From Continuously Moving Satellite</title>
    <url>/Needles%20in%20a%20Haystack%20Tracking%20City-Scale%20Moving%20Vehicles%20From%20Continuously%20Moving%20Satellite.html</url>
    <content><![CDATA[<p>将差分结果认定为目标+噪声，形态学去掉规则噪声。</p>
<a id="more"></a>

<h2 id="卫星视频移动目标检测问题"><a href="#卫星视频移动目标检测问题" class="headerlink" title="卫星视频移动目标检测问题"></a>卫星视频移动目标检测问题</h2><ol>
<li><p>目标移动并且很小，通常只有几个像素，并且没有纹理和颜色，最好的特征就是运动特征。</p>
</li>
<li><p>卫星视频的帧覆盖了一个巨大范围并且提供一个动态场景。</p>
<p>文章根据镜头与物体的距离将视频分为near-field, medium-field, far-field surveillance videos, and extremely far-field satellite videos[2] [3].卫星视频的处理更为困难，不仅是广阔视野还因为非常复杂的背景。</p>
</li>
<li><p><strong>背景称亚像素级不均匀运动</strong></p>
<p>光流场如[4],[5]所示，背景的运动导致静止像素也在动，从而为运动目标检测造成妨碍。</p>
</li>
</ol>
<h2 id="方案概述"><a href="#方案概述" class="headerlink" title="方案概述"></a>方案概述</h2><p>将每帧分解成两部分，原始图像和随机2D噪声信号图。采用随机分布拟合噪声，有利于分辨潜在的运动目标。</p>
<p>a local tactic is applied to address intra-variants within a frame and discern inter-variants between frames</p>
<p>然后是一种基于多形态线索的区域生长方法和判别方法可以消除其他噪声。</p>
<p>然后使用KF追踪车辆</p>
<h2 id="related-work"><a href="#related-work" class="headerlink" title="related work"></a>related work</h2><p>B.移动目标检测和追踪</p>
<p>GMM[14-16], Vibe[17], Hierarchical convolutional features(HCF)[18,19]</p>
<p>GMM利用权重呼和的高斯分布来建模像素值随时间的变化，但是像素值变化可能不是服从于高斯分布，因此大多数情况下我们不能用一个确定的参数模型表示像素值变化</p>
<p>Vibe将不同时间不长的像素值认为是一个空间的样本，为了表示这些像素。</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="Key-concept："><a href="#Key-concept：" class="headerlink" title="Key concept："></a>Key concept：</h3><p>==<em>detector</em>==: 集成了local tactic and noise modeling algorithm。</p>
<p>==<em>candidates</em>==：detector的输出，包括了true和some noise</p>
<p>==<em>discrimination</em>==：判别true和noise的程序，包括the proposed region growing and multi-morphological-cue based discrimination algorithms.</p>
<p>==<em>Hypotheses</em>==: discrimination 的输出，由true和few noise组成</p>
<p>==<em>state</em>==：a vector that includes <strong>position, velocity, and acceleration</strong> of a vehicle in a time step</p>
<p>==<em>track</em>==: a sequence of states of a vehicle in the temporal domain. 被一个独特ID标记并安排给一个KF</p>
<p>==<em>association</em>==：匹配算法，meets the minimum cost</p>
<p>==<em>prediction</em>==： KF推断出的当前位置</p>
<p><img src="/Needles%20in%20a%20Haystack%20Tracking%20City-Scale%20Moving%20Vehicles%20From%20Continuously%20Moving%20Satellite/image-20201117104616406.png" alt="image-20201117104616406"></p>
<h3 id="Motion-Based-Detection-Using-Local-Noise-Modeling"><a href="#Motion-Based-Detection-Using-Local-Noise-Modeling" class="headerlink" title="Motion-Based Detection Using Local Noise Modeling"></a>Motion-Based Detection Using Local Noise Modeling</h3><p>二值化的阈值不能固定，需要自适应</p>
<ol>
<li><p>local tactic： 解决帧内的剧烈变化，在帧中沿垂直和水平方向实现2D光栅化，每个块设为30*30（emparically)</p>
</li>
<li><p>Detecting method:</p>
</li>
</ol>
<p><em>Step1</em>: 差分</p>
<p><em>Step2</em>: 估计噪声分布</p>
<p><em>Step3</em>：二值化</p>
<p><em>Step4</em>：逻辑与</p>
<p><img src="/Needles%20in%20a%20Haystack%20Tracking%20City-Scale%20Moving%20Vehicles%20From%20Continuously%20Moving%20Satellite/image-20201117105120909.png" alt="image-20201117105120909"></p>
<p>3)具体解释</p>
<p><strong><em>Step1:差分</em></strong></p>
<p>将帧看做2d信号，由原始光信号和附加的随机噪声组成<br>$$<br>G_i(x,y)=g_i(x,y)+n_i(x,y)<br>$$</p>
<blockquote>
<p>RGB-&gt;Grey</p>
</blockquote>
<p>then,<br>$$<br>D_{i,i+k}(x,y)=|G_i(x,y)-G_{i+k}(x,y)|<br>=|n_i(x,y)-n_{i+k}(x,y)|<br>$$<br><strong><em>Step2:估计噪声分布</em></strong></p>
<p>根据直方图，heavy tail对应outlier，是target；拟合方法采用指数分布<br>$$<br>c_E(x;\lambda)=\begin{cases}<br>1-exp(-\lambda x), &amp;\ x&gt;0 \<br>0,&amp;x \leq \mbox{0}\end{cases}<br>$$<br><strong><em>Step3:<strong>**</strong>二值化</em></strong></p>
<p>根据[7]，引入一个预定义的概率值$p_{fa}$来推导阈值<br>$$<br>th=c_E^{-1}(1-p_{fa};\lambda)<br>$$<br>-1代表逆函数，这里设置参数$p_{fa}$为0.05，大于阈值th被认为是outlier</p>
<p><strong><em>Step4：逻辑与</em></strong></p>
<p>In addition to eliminating ambiguities, logical AND also reduces the existing noises due to their random appearing.</p>
<h3 id="Region-Growing-and-Multi-Morphological-Cue-Based-Discrimination"><a href="#Region-Growing-and-Multi-Morphological-Cue-Based-Discrimination" class="headerlink" title="Region Growing and Multi-Morphological-Cue Based Discrimination"></a>Region Growing and Multi-Morphological-Cue Based Discrimination</h3><p>仍然可能有噪声，包括不规则噪声和规则噪声，不规则噪声是由剧烈光照变化或轻微偏差引起的，随机出现在一些连续帧中，<strong>可以被KF逐步消除</strong></p>
<p>另一种是规则噪声，背景均匀移动，他是与移动目标混淆的重要一点。</p>
<p>用一个术语<em>pseudo motion</em>代称</p>
<p>算法：</p>
<p>1.车辆是2D时域中的一个奇异点，相反的，这些规则噪声的邻域内相似分布，如果<em>candidates</em>可以被链接到相似的邻域像素，那就可以根据形状区分，车辆是矩形，噪声是奇怪形状</p>
<p>邻域被定义为11*11</p>
<p>相似度度量使用高斯分布</p>
<p>被确定连通的像素的阈值应该是<br>$$<br>th_G^-=c_G^-(p_{fa}^-\mu,\sigma)\<br>th_G^+=c_G^{-1}(p_{fa}^+\mu,\sigma)<br>$$<br>两个参数选定为0.005和1-0.005</p>
<p>2.区域生长后，采用一系列形态学特征来区分车辆目标和噪声。</p>
<p>这些特征包括（candidate直接叫做候选了）</p>
<p><em>Area</em>：候选的面积</p>
<p><em>Extent</em>：候选的像素与候选框像素比</p>
<p><em>Major Axis Length</em>：如果一个椭圆与候选连接区域的标准化第二中心相同，那么椭圆半长轴定义为主轴长</p>
<p><em>Eccentricity</em>：上述椭圆的离心率</p>
<p>面积和长轴衡量大小，范围和离心率衡量候选与矩形的相似性。</p>
<p>注意，视频的车当然是刚体</p>
<h2 id="Metric"><a href="#Metric" class="headerlink" title="Metric"></a>Metric</h2><p><strong><em>Precision</em></strong><br>$$<br>P=\frac{TP}{TP+FP}<br>$$<br>TP：检测出的，并且是目标</p>
<p>FP：检测出的，不是目标</p>
<p>FN：没检测出的目标</p>
<p><strong><em>Recall</em></strong><br>$$<br>R=\frac{TP}{TP+FN}<br>$$<br><strong><em>F1-score</em></strong><br>$$<br>F_1=2\frac{P*R}{P+R}<br>$$</p>
<hr>
<p> <strong><em>Jaccard similarity</em></strong><br>$$<br>J=\frac{TP}{TP+FP+FN}<br>$$<br><strong><em>MOTA</em></strong><br>$$<br>MOTA=1-\frac{\sum_i(FN_i+FP_i+IDSW_i)}{\sum_iGT_i}<br>$$<br>IDSW：ID switch</p>
<p>指标范围从负无穷到1，越大越好</p>
<p><strong><em>MOTP</em></strong><br>$$<br>MOTP=\frac{\sum_iIoU_i}{\sum_iM_i}<br>$$<br>从0到1</p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3><h4 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h4><h4 id="Competitor："><a href="#Competitor：" class="headerlink" title="Competitor："></a>Competitor：</h4><p>GMM、ViBe、HCF，+KF</p>
<h4 id="GT"><a href="#GT" class="headerlink" title="GT"></a>GT</h4><p>考虑到工作量，标注了随机抽取的三个500*500区域</p>
<p>每10帧标记一下</p>
<p>其他帧的情况通过线性插值得到</p>
<p>使用Ground Truth Labeler App inMATLAB 2018a </p>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>传统方法</category>
      </categories>
      <tags>
        <tag>移动目标检测</tag>
        <tag>传统方法</tag>
        <tag>背景差分法</tag>
      </tags>
  </entry>
  <entry>
    <title>On Learning Vehicle Detection in Satellite Video</title>
    <url>/On-Learning-Vehicle-Detection-in-Satellite-Video.html</url>
    <content><![CDATA[<p>通过一个简单网络提取特征并转化为热力图，然后进行阈值分割和最终预测。</p>
<a id="more"></a>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>航空和卫星图像的分辨率和数据量都很大，例如广域运动图像WAMI提供每帧400MP和每秒三帧。卫星图像30FPS的4K RGB图像，并且可能覆盖几十平方公里，每张图中有上千个小物体并且上百个种类。根据GSD，卫星图像中的目标可能只有10px量级。</p>
<p>目标检测变得非常模糊，对噪声和干扰非常敏感，搜索空间急剧增加，变得非常稀疏。人甚至都看不出来物体，这也造成数据标注的大问题。因此，目前研究主要依靠背景差分和帧差法。 [16, 28, 32, 18, 4, 3]</p>
<p>深度学习的方法都是用不同场景的狭窄数据集进行测试的，这也使得结果有效性受到质疑。</p>
<p>因此本文研究了卫星图像中的车辆检测问题，SkySat-1卫星提供120s，30Hz，2K全色视频，面积$1.1km^2$，GSD80cm。Jilin-1提供4MP像素的彩色视频。</p>
<p>这是第一次使用神经网络和深度学习来直接回归卫星视频中车辆位置。受最近在WAMI上工作的启发，文章提出一种开发时间一致性的深度学习方法。为了克服数据问题，将WAMI的工作迁移学习到卫星视频中。</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>在滑窗顶部使用分类器是一种可能的路径。但是还没有，已有的航空图像方法拿来用都不好。</p>
<p>另一个想法是对车辆和背景进行像素级分类（语义分割）[23]将Inception和Resnet结合给出一个热力图，然后选取固定车辆大小和非极大值抑制得到结果。</p>
<p>如何使用时空信息是一个重要问题，The standard is to use background subtraction (BGS) [35, 16, 28, 32, 1] and frame differencing(FD)[18,4,3]。另外，[2]提出将YOLO和时空滤波器结合，[22]提出将KLT跟踪用在视频上，with a SegNet on overlapping multispectral data。[35]使用CIFAR训练的resnet分类器去处理混合高斯前景模型的结果。这里的标准是应用连接成分分析[16，28]，显著性分析，分割[32，18]，分布拟合[4，3]，然后是形态学。</p>
<p>还有一个大问题是稀疏性，如在WAMI中，通过将大图像进行聚类再去做。</p>
<p>最近，[10，9，26，12]也考虑了飞机、火车和车辆的跟踪，或者使用光流[10，9]，相关跟踪器（KLT）[26]或相关和卡尔曼滤波器的组合[12]。</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>受[17]启发，他设计了两个神经网络，称为ClusterNet和FoveaNet，检测WAMI中的移动目标。ClusterNet提出region of object(ROOBI)，基于areas of interest(AOI)，作为FoveaNet的输入。文章没有使用ClusterNet，而是直接将AOI裁剪成128*128的ROOBI。</p>
<h3 id="3-1-FoveaNet-and-thresholding"><a href="#3-1-FoveaNet-and-thresholding" class="headerlink" title="3.1 FoveaNet and thresholding"></a>3.1 FoveaNet and thresholding</h3><p><img src="/On-Learning-Vehicle-Detection-in-Satellite-Video/image-20201121162541315.png" alt="image-20201121162541315"></p>
<p>这个网络由全卷积层构成，每层滤波器的数量分别是32,32,32,256,512,256,256 and 1。</p>
<p>第一层后接一个2*2池化层，第6,7层加了0.5dropout，最后一层每个神经元在像素级对移动车辆可能性投票。</p>
<p>输入网络的是一个$N\times N\times c$的帧栈，N是ROOBI size，c是连续帧。CNN应该学到检测中心帧的目标位置。文章举例c=5。</p>
<p>地面真实情况基于热图H，该热图是通过叠加高斯分布创建的，其中每个分布的中心是图像中车辆的像素位置，其中n是像素位置提供的下采样地面真实坐标，σ是高斯模糊的方差。在训练过程中，网络学习最小化网络输出和生成的地面真实热图之间的欧几里德距离</p>
<p>第二步处理预测的热图以确定物体的位置。为此，通过OTSU阈值化将热图转换为分割图[17]。如果分割区域大于阈值α，则区域中心被定义为对象位置。</p>
<h3 id="3-2-迁移学习"><a href="#3-2-迁移学习" class="headerlink" title="3.2 迁移学习"></a>3.2 迁移学习</h3><p>the WPAFB images have about four times higher GSD than the LasVegas video，为此，我们根据WPAFB数据集训练CNN。之后我们对CNN卫星视频数据进行微调。</p>
<h2 id="Experimental-Evaluation-and-Results"><a href="#Experimental-Evaluation-and-Results" class="headerlink" title="Experimental Evaluation and Results"></a>Experimental Evaluation and Results</h2><table>
<thead>
<tr>
<th>参数</th>
<th>值</th>
</tr>
</thead>
<tbody><tr>
<td>batch size</td>
<td>32</td>
</tr>
<tr>
<td>lr</td>
<td>1e-5</td>
</tr>
</tbody></table>
<p>数据准备包括帧注册和相机运动补偿。</p>
<p>三组实验，第一组复现17的结果，第二组降低WPAFB的GSD，这样就和卫星图像相似，第三组，微调并评估结果。</p>
<p>标准：检测结果在gt的一定距离$\theta$内就认为TP，如果范围内有多个结果，最近的当做TP，剩下的如果没有被确定为其他TP就是FP，没在范围内的自然也是FP，范围内一个检测结果也没有的叫做FN。结果将按照P,R,F1比较。</p>
<p>选取与17一样的AOI，编号为34,40,41。AOI 40在一个主要交叉口处有大量密集交通，而AOI 41主要由道路上的单车组成。AOI34是两种交通模式的组合。训练就选其中两个训练另一个。</p>
<p>如果目标在5帧内移动$\omega$像素，记为移动目标。</p>
<h3 id="4-1-Experiment-1-Baseline-evaluation"><a href="#4-1-Experiment-1-Baseline-evaluation" class="headerlink" title="4.1 Experiment 1: Baseline evaluation"></a>4.1 Experiment 1: Baseline evaluation</h3><p>复现17结果。</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>值</th>
</tr>
</thead>
<tbody><tr>
<td>N(ROOBI edge length)</td>
<td>100px</td>
</tr>
<tr>
<td>$\sigma$(variance of Gaussian blur)</td>
<td>2</td>
</tr>
<tr>
<td>$\theta$(evaluation threshold)</td>
<td>40px</td>
</tr>
<tr>
<td>$\omega$(threshold for removing stationary cars)</td>
<td>15px</td>
</tr>
<tr>
<td>$\alpha$(threshold to disregard small segments)</td>
<td>15px</td>
</tr>
</tbody></table>
<h3 id="4-2-Experiment-2-调整GSD"><a href="#4-2-Experiment-2-调整GSD" class="headerlink" title="4.2 Experiment 2: 调整GSD"></a>4.2 Experiment 2: 调整GSD</h3><p>在第二个实验中，我们用0.4和0.2的比例因子（SF）来降低图像，结果分别是原始图像分辨率的40%和20%。我们选择的SF为0.2，因为这个因子将WPAFB数据集中的典型车辆对象大小从18×9px减小到3.6×1.8px，这与卫星视频中的车辆大小类似。实验设置了以下参数：SF=0.4，N=100px，σ=2，θ=16px，ω=6px，α=15px，SF=0.2，N=100px，σ=1，θ=8px，ω=3px，α=3.5px</p>
<p>结果表明，随着GSD变大，效果显著变差，但是c变大会有效提高分数。</p>
<p>另外，结果有严重的黏连问题，文章没有很好的处理，只是妥协性的降低了$\sigma$。</p>
<h3 id="4-3-Experiment-3-Satellite-video"><a href="#4-3-Experiment-3-Satellite-video" class="headerlink" title="4.3 Experiment 3: Satellite video"></a>4.3 Experiment 3: Satellite video</h3><p>对于训练和评估，我们设置θ=8px，α=4px，σ=1，c=5和N=128px。另外，我们设置SF=0.2和ω=3px来训练WPAFB数据集。本实验观察到用leaky ReLUs代替ELUs，提高了训练效率。</p>
<p>[17]关于空间上下文信息需要在第一层中使用大过滤器的观点似乎是误导性的，因为上下文是由网络的接受场引入深层网络的高层的。我们认为，滤波器的大小取决于车辆在连续帧中的像素距离，这样时空网络就可以利用时间信息，这是我们实验所证实的</p>
<p>结果还表明，学习检测移动目标通过线性运动的斜率比学习视觉外观的时空变化简单。预训练对泛化非常重要。</p>
<p>最后，关于帧率的实验表明高帧率对结果没有大影响，因为需要学习的特征很简单。</p>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>深度学习方法</category>
      </categories>
      <tags>
        <tag>移动目标检测</tag>
        <tag>深度学习方法</tag>
        <tag>2020</tag>
      </tags>
  </entry>
  <entry>
    <title>Error Bounded Foreground and Background Modeling for Moving Object Detection in Satellite Videos</title>
    <url>/Error-Bounded-Foreground-and-Background-Modeling-for-Moving-Object-Detection-in-Satellite-Videos.html</url>
    <content><![CDATA[<p>暂时弃坑，有时间看稀疏学习后再来</p>
<a id="more"></a>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>考虑到摄像机运动较小，帧之间的背景在时间上是稳定且相似的，因此假设他们位于低维子空间中，该子空间通常使用低秩矩阵分解进行估计。经典的主成分分析PCA方法通过寻求估计背景的秩最小化来生成背景，但对严重破坏的像素十分敏感。运动对象只占视频场景的一小部分，前景是非常稀疏的。鲁棒PCA算法将视频分解为低秩背景和稀疏前景矩阵，主成分追踪和快速低秩近似是为了解决低秩矩阵分解问题产生的。但是对前景的噪声比较大时，效果下降。</p>
<p>一阶马尔科夫场MRF因此用来限制估计前景的光滑边缘从而降低噪声影响。另一种方法是正则化空间相邻像素组的稀疏性。低秩和结构化稀疏分解LSD框架利用结构化稀疏诱导范数将感兴趣对象的空间先验信息整合到低秩矩阵分解中，当LSD将视频帧分解为低秩前=背景和稀疏前景是，通常有很大的残差，从而使背景的秩上升或者前景出现噪声。</p>
<p>卫星图像中物体和随机噪声很有相似度，直接使用LSD不行，本文提出E-LSD方法，利用不等式约束将视频分解，使得残差在噪声水平之下有边界。求解E-LSD公式具有挑战性，通过引入一种替代方法ADMM提供一种解决方案。并且加速了收敛。</p>
<h2 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h2><h3 id="2-1-结构化稀疏诱导范数"><a href="#2-1-结构化稀疏诱导范数" class="headerlink" title="2.1 结构化稀疏诱导范数"></a>2.1 结构化稀疏诱导范数</h3><p>结构化稀疏诱导范数在一组给定变量上积分先验结构，为了编码相关变量的先验结构，根据变量之间的关系将整个变量集分成几个组，并在组的层次上定义结构化稀疏性。</p>
<hr>
<p>看不懂，暂弃坑</p>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>传统方法</category>
      </categories>
      <tags>
        <tag>移动目标检测</tag>
        <tag>传统方法</tag>
        <tag>低秩矩阵分解</tag>
      </tags>
  </entry>
  <entry>
    <title>Motion Detection in Satellite Video</title>
    <url>/Motion-Detection-in-Satellite-Video.html</url>
    <content><![CDATA[<p>完整介绍了一种背景差分法的流程，对伪运动目标和鬼影判断都有算法。</p>
<a id="more"></a>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>背景差分的关键是采用合适的像素采样方法建立背景模型。根据前一帧检测的分割信息进行恰当的模型更新，以适应背景变化。[4]</p>
<p>文章归纳的挑战有：</p>
<p>1）由于卫星的运行、抖动等原因，卫星图像总有微小变化。变化分为两部分，全局变化和由视差引起的局部伪运动。</p>
<p>2）帧宽度很大，分辨率和对比度都较低，运动物体的尺寸很小且缺少特征信息。</p>
<p>因此，</p>
<p>1）对于大宽度动态场景，要对全局运动加以补偿，每一帧的背景应该是固定的。</p>
<p>2）建立准确而稳健的背景模型，并分割和提取小物体，另外针对经典的补偿方法需要加以改进。</p>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>1.利用中间帧建立背景模型</p>
<p>2.估计真贱运动模型，并以此为基础做运动补偿，通过比较补偿好的帧与背景帧来得到目标的分割结果。</p>
<p>3.应用连接分量分析的方法提取潜在目标，根据检测到的分割信息和候选的伪运动区域更新背景模型，来处理下一帧。</p>
<p><img src="/Motion-Detection-in-Satellite-Video/image-20201122152805262.png" alt="image-20201122152805262"></p>
<h2 id="具体各部分算法"><a href="#具体各部分算法" class="headerlink" title="具体各部分算法"></a>具体各部分算法</h2><h3 id="3-1-改进ViBe背景模型"><a href="#3-1-改进ViBe背景模型" class="headerlink" title="3.1 改进ViBe背景模型"></a>3.1 改进ViBe背景模型</h3><p>本文的背景模型用ViBe法，仅在一帧的基础上完成模型建立。</p>
<p>考虑到局部伪运动，对vibe模型进行调整，引入更新因子<strong>a</strong>，表示背景模型的更新频率。</p>
<h3 id="3-2-全局运动补偿"><a href="#3-2-全局运动补偿" class="headerlink" title="3.2 全局运动补偿"></a>3.2 全局运动补偿</h3><p>本章对全局运动进行补偿。本文提出一种A global motion compensation method of blocked  forward-backward LK optical flow。</p>
<p>首先，对每个block使用uniform blocking processing方法提取出Good Feartures。</p>
<p>然后，利用光流跟踪跟踪Good Feature，实现了相关点集的匹配，再根据模型中相关点集估计每个block的运动模型。最后，收集整理变换结果完成整帧的运动补偿。</p>
<h4 id="图像分块和Good-Feature提取"><a href="#图像分块和Good-Feature提取" class="headerlink" title="图像分块和Good Feature提取"></a>图像分块和Good Feature提取</h4><p>将参考帧打成$M\times N$块，每个小块记为$B_{mn}$，第m行第n列个块。然后使用[15]的Good Feature方法，用一个3,3滤波窗。最小因子和距离的取值可以设置得稍小，保证在低频弱特征的区域提取出足够多的好特征，且分布均匀。</p>
<p><img src="/Motion-Detection-in-Satellite-Video/image-20201122154556809.png" alt="image-20201122154556809"></p>
<h4 id="前后LK光流的点跟踪与匹配"><a href="#前后LK光流的点跟踪与匹配" class="headerlink" title="前后LK光流的点跟踪与匹配"></a>前后LK光流的点跟踪与匹配</h4><p>理想情况下向前跟踪一步再回来应该回到同一个地方，但是实际上会有误差，这个误差就是要消掉或者说控制的。</p>
<p><img src="/Motion-Detection-in-Satellite-Video/image-20201122155051112.png" alt="image-20201122155051112"></p>
<p><img src="/Motion-Detection-in-Satellite-Video/image-20201122155418650.png" alt="image-20201122155418650"></p>
<h4 id="模型估计和运动补偿"><a href="#模型估计和运动补偿" class="headerlink" title="模型估计和运动补偿"></a>模型估计和运动补偿</h4><p>匹配完成后，利用相关点估计block的帧间2D仿射模型参数并建立集合变换关系。同时，the MSACM-estimator Sample Consensus algorithm [16] 可以用来消除有误差的匹配点，进一步优化精度。2维帧间仿射参数矩阵如下：<br>$$<br>\begin{bmatrix}<br>a_1, a_2, 0\<br>a_3, a_4, 0\<br>a_5, a_6, 1\<br>\end{bmatrix}<br>$$<br>然后，逆解的数字微分校正方法被使用[17]。一个空白图片被作为补偿帧建起来，根据变换关系，原补偿帧的小块就按照变换关系到空白帧上。这就完成了全局补偿。</p>
<p><img src="/Motion-Detection-in-Satellite-Video/image-20201122160332829.png" alt="image-20201122160332829"></p>
<h4 id="运动检测和分割"><a href="#运动检测和分割" class="headerlink" title="运动检测和分割"></a>运动检测和分割</h4><p>使用ViBe法建立背景模型后的运动检测就是那一套，当所有移动像素都分类玩，前北京的二值图像就获得了，在此基础上进行连通成分分析，将每个连通分量当做候选并进行分割。</p>
<h4 id="判断伪运动和背景模型更新"><a href="#判断伪运动和背景模型更新" class="headerlink" title="判断伪运动和背景模型更新"></a>判断伪运动和背景模型更新</h4><p>分割出的候选目标由3个主要部分构成，真实运动目标，视差伪目标和归目标。伪运动的判断目的在于找到后两种</p>
<p>首先，对于时差伪目标，选取准则如下：1）利用连通分量节点的坐标，提取目标的最小包围矩形。计算长宽比，如果长宽比大于3.5，则使该目标成为潜在的“伪运动”。2）  从目标候选区域提取边缘。确认靶标与边沿的重叠面积是否超过靶标面积的90%。如果是这样，那么目标就可以被视为潜在的“伪运动”，</p>
<p>根据这设置伪目标区域：将潜在“伪运动”目标的包络矩形向外扩展1个像素，同时向运动模型的平移矢量方向向外扩展10个像素，得到更新因子矩形A。</p>
<p><img src="/Motion-Detection-in-Satellite-Video/image-20201122164026047.png" alt="image-20201122164026047"></p>
<p>根据“鬼”目标的特点，确定了判断方法：记录运动目标第一次出现的位置。如果运动目标连续10帧静止，则视为潜在的“幽灵”目标</p>
<p>然后对背景进行更新，更新策略如下：每个像素背景模型都有一个更新因子a，每个像素背景模型的更新频率是1/a，</p>
<p><img src="/Motion-Detection-in-Satellite-Video/image-20201122164318201.png" alt="image-20201122164318201"></p>
<p>这种更新策略保证伪目标都进入背景中</p>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>传统方法</category>
      </categories>
      <tags>
        <tag>移动目标检测</tag>
        <tag>传统方法</tag>
        <tag>背景差分法</tag>
        <tag>光流法</tag>
        <tag>2017</tag>
      </tags>
  </entry>
  <entry>
    <title>Small Moving Vehicle Detection in a Satellite Video of an Urban Area</title>
    <url>/Small-Moving-Vehicle-Detection-in-a-Satellite-Video-of-an-Urban-Area.html</url>
    <content><![CDATA[<p>大概是第一篇，主要方法是对前景运动分割和轨迹积累，构建热力图，并以此提出显著性背景模型。</p>
<a id="more"></a>

<h2 id="算法框架"><a href="#算法框架" class="headerlink" title="算法框架"></a>算法框架</h2><p>2-step：</p>
<p>step1：建立运动热力图</p>
<p>step2：基于局部显著性的背景模型</p>
<p><img src="/Small-Moving-Vehicle-Detection-in-a-Satellite-Video-of-an-Urban-Area/image-20201123093520970.png" alt="image-20201123093520970"></p>
<h3 id="2-1-热力图构建"><a href="#2-1-热力图构建" class="headerlink" title="2.1 热力图构建"></a>2.1 热力图构建</h3><p>运动车辆是在固定区域里形式的，包括不同的高速公路和狭窄道路。如果实现对运动区域进行分割，则可以有效的抑制误检测。热力图构建就是这个目的。</p>
<h4 id="2-1-1-运动检测"><a href="#2-1-1-运动检测" class="headerlink" title="2.1.1 运动检测"></a>2.1.1 运动检测</h4><p>文章没有对背景进行补偿，他觉得不需要。直接用ViBe法建立背景模型。</p>
<h4 id="2-1-2-基于轨迹的虚警过滤器"><a href="#2-1-2-基于轨迹的虚警过滤器" class="headerlink" title="2.1.2 基于轨迹的虚警过滤器"></a>2.1.2 基于轨迹的虚警过滤器</h4><p>将每个连通域认为是一个检测结果，有时可以用形态来判别，但是任务目标的大小普遍和噪声差不多大，文章提出使用轨迹判断是真是假。</p>
<p>通过匈牙利算法提取轨迹，匈牙利算法即：</p>
<p>假设第t帧有m个检测结果，第t+1帧有n个检测结果，两帧的目标应该有一对一关系。记$C_{ij}$是第t帧的第i个目标与第（t+1)帧第j个目标的欧氏距离，$X_{ij}$是其关联关系，如果判定这两个目标是同一个目标，那么$X_{ij}$就是1。所以任务就是最小化总距离。<br>$$<br>z = min\sum_{i=1}^m \sum_{j=1}^nC_{ij}X_{ij}<br>$$<br>如果m&lt;n，后帧检测的多，没匹配上的检测当做新的；如果m&gt;n，那么未关联的目标可能是真的也可能是假的。真目标的轨迹在时域上应该是连续的，因此保留长度大于阈值长度$T_{length}$的轨迹，并且轨迹方向稳定。</p>
<h4 id="2-1-3-运动热力图"><a href="#2-1-3-运动热力图" class="headerlink" title="2.1.3 运动热力图"></a>2.1.3 运动热力图</h4><p>应用距离变换建立热力图，距离变换是二值图中一种特殊变换，会产生一张灰度图。每一个点到距离的最短路径长度就是结果，并设置了一个阈值$T_{heat}$为15</p>
<h3 id="2-2-局部显著性背景模型"><a href="#2-2-局部显著性背景模型" class="headerlink" title="2.2 局部显著性背景模型"></a>2.2 局部显著性背景模型</h3><h4 id="2-2-1-建立局部显著图"><a href="#2-2-1-建立局部显著图" class="headerlink" title="2.2.1 建立局部显著图"></a>2.2.1 建立局部显著图</h4><p>在滑窗的基础上计算显著性<br>$$<br>Sal(k)=\sum_{\forall i \in \omega}\lVert I_k - I_i\rVert<br>$$<br>其中，$I_i$是像素值，$\omega$是滑窗，这个意思也就是与相邻点对比的灰度情况，数越大越显著，经验公式里滑窗大小为3。</p>
<p>对每个点都这样做，就得到了显著图。</p>
<h4 id="2-2-2-基于显著性的运动检测"><a href="#2-2-2-基于显著性的运动检测" class="headerlink" title="2.2.2 基于显著性的运动检测"></a>2.2.2 基于显著性的运动检测</h4><p>在显著图的基础上使用ViBe</p>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>传统方法</category>
      </categories>
      <tags>
        <tag>移动目标检测</tag>
        <tag>传统方法</tag>
        <tag>ViBe</tag>
        <tag>热力图</tag>
        <tag>2016</tag>
      </tags>
  </entry>
</search>
