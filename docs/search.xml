<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>GMM</title>
    <url>/GMM.html</url>
    <content><![CDATA[<h1 id="GMM-Gaussian-Mixture-Model"><a href="#GMM-Gaussian-Mixture-Model" class="headerlink" title="GMM-Gaussian Mixture Model"></a>GMM-Gaussian Mixture Model</h1><a id="more"></a>

<h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>参数概率密度函数，用gaussian component densities的加权和表示。</p>
<p>GMM参数估计是从训练数据使用迭代期望最大化（EM）算法或最大后验（MAP）估计从一个训练有素的先验模型。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>$$<br>p(\text{x}|\lambda)=\sum_{i=1}^M\omega_ig(\text{x}|\mu_i,\begin{matrix}\sum_i\end{matrix})<br>$$</p>
<p>where<br>$$<br>g(\text{x}|\mu_i,\begin{matrix}\sum_i\end{matrix})=\frac{1}{(2\pi)^{D/2}|\begin{matrix}\sum_i\end{matrix}|^{1/2}}\text{exp} { -\frac{1}{2}(\text{x}-\mu_i)’\begin{matrix}\sum_i^{-1}\end{matrix}(\text{x}-\mu_i)}<br>$$<br>with mean vextor $\mu_i$ and covariance matrix $\sum_i$, the weight satisfy the constraint that $\sum_{i=1}^M\omega_i=1$.</p>
<p>in the formular, the parameters can be denoted by<br>$$<br>\lambda={ \omega_i,\mu_i,\begin{matrix}\sum_i\end{matrix}}\qquad i=1,2,…,M<br>$$<br>then, </p>
<img src="/GMM/a1.png" class="" title="This is a picture">

<blockquote>
<p>Fig. 1. Comparison of distribution modeling. (a) histogram of a single cepstral coefficient from a 25 second utterance by a male speaker (b)maximum likelihood uni-modal Gaussian model (c) GMM and its 10 underlying component densities (d) histogram of the data assigned to the VQ centroid locations of a 10 element codebook.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>Needles in a Haystack Tracking City-Scale Moving Vehicles From Continuously Moving Satellite</title>
    <url>/Needles%20in%20a%20Haystack%20Tracking%20City-Scale%20Moving%20Vehicles%20From%20Continuously%20Moving%20Satellite.html</url>
    <content><![CDATA[<h1 id="20201117-Needles-in-a-Haystack-Tracking-City-Scale-Moving-Vehicles-From-Continuously-Moving-Satellite"><a href="#20201117-Needles-in-a-Haystack-Tracking-City-Scale-Moving-Vehicles-From-Continuously-Moving-Satellite" class="headerlink" title="20201117-Needles in a Haystack: Tracking City-Scale Moving Vehicles From Continuously Moving Satellite"></a>20201117-Needles in a Haystack: Tracking City-Scale Moving Vehicles From Continuously Moving Satellite</h1><a id="more"></a>

<h2 id="卫星视频移动目标检测问题"><a href="#卫星视频移动目标检测问题" class="headerlink" title="卫星视频移动目标检测问题"></a>卫星视频移动目标检测问题</h2><ol>
<li><p>目标移动并且很小，通常只有几个像素，并且没有纹理和颜色，最好的特征就是运动特征。</p>
</li>
<li><p>卫星视频的帧覆盖了一个巨大范围并且提供一个动态场景。</p>
<p>文章根据镜头与物体的距离将视频分为near-field, medium-field, far-field surveillance videos, and extremely far-field satellite videos[2] [3].卫星视频的处理更为困难，不仅是广阔视野还因为非常复杂的背景。</p>
</li>
<li><p><strong>背景称亚像素级不均匀运动</strong></p>
<p>光流场如[4],[5]所示，背景的运动导致静止像素也在动，从而为运动目标检测造成妨碍。</p>
</li>
</ol>
<h2 id="方案概述"><a href="#方案概述" class="headerlink" title="方案概述"></a>方案概述</h2><p>将每帧分解成两部分，原始图像和随机2D噪声信号图。采用随机分布拟合噪声，有利于分辨潜在的运动目标。</p>
<p>a local tactic is applied to address intra-variants within a frame and discern inter-variants between frames</p>
<p>然后是一种基于多形态线索的区域生长方法和判别方法可以消除其他噪声。</p>
<p>然后使用KF追踪车辆</p>
<h2 id="related-work"><a href="#related-work" class="headerlink" title="related work"></a>related work</h2><p>B.移动目标检测和追踪</p>
<p>GMM[14-16], Vibe[17], Hierarchical convolutional features(HCF)[18,19]</p>
<p>GMM利用权重呼和的高斯分布来建模像素值随时间的变化，但是像素值变化可能不是服从于高斯分布，因此大多数情况下我们不能用一个确定的参数模型表示像素值变化</p>
<p>Vibe将不同时间不长的像素值认为是一个空间的样本，为了表示这些像素。</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="Key-concept："><a href="#Key-concept：" class="headerlink" title="Key concept："></a>Key concept：</h3><p>==<em>detector</em>==: 集成了local tactic and noise modeling algorithm。</p>
<p>==<em>candidates</em>==：detector的输出，包括了true和some noise</p>
<p>==<em>discrimination</em>==：判别true和noise的程序，包括the proposed region growing and multi-morphological-cue based discrimination algorithms.</p>
<p>==<em>Hypotheses</em>==: discrimination 的输出，由true和few noise组成</p>
<p>==<em>state</em>==：a vector that includes <strong>position, velocity, and acceleration</strong> of a vehicle in a time step</p>
<p>==<em>track</em>==: a sequence of states of a vehicle in the temporal domain. 被一个独特ID标记并安排给一个KF</p>
<p>==<em>association</em>==：匹配算法，meets the minimum cost</p>
<p>==<em>prediction</em>==： KF推断出的当前位置</p>
<p><img src="/Needles%20in%20a%20Haystack%20Tracking%20City-Scale%20Moving%20Vehicles%20From%20Continuously%20Moving%20Satellite/image-20201117104616406.png" alt="image-20201117104616406"></p>
<h3 id="Motion-Based-Detection-Using-Local-Noise-Modeling"><a href="#Motion-Based-Detection-Using-Local-Noise-Modeling" class="headerlink" title="Motion-Based Detection Using Local Noise Modeling"></a>Motion-Based Detection Using Local Noise Modeling</h3><p>二值化的阈值不能固定，需要自适应</p>
<ol>
<li><p>local tactic： 解决帧内的剧烈变化，在帧中沿垂直和水平方向实现2D光栅化，每个块设为30*30（emparically)</p>
</li>
<li><p>Detecting method:</p>
</li>
</ol>
<p><em>Step1</em>: 差分</p>
<p><em>Step2</em>: 估计噪声分布</p>
<p><em>Step3</em>：二值化</p>
<p><em>Step4</em>：逻辑与</p>
<p><img src="/Needles%20in%20a%20Haystack%20Tracking%20City-Scale%20Moving%20Vehicles%20From%20Continuously%20Moving%20Satellite/image-20201117105120909.png" alt="image-20201117105120909"></p>
<p>3)具体解释</p>
<p><strong><em>Step1:差分</em></strong></p>
<p>将帧看做2d信号，由原始光信号和附加的随机噪声组成<br>$$<br>G_i(x,y)=g_i(x,y)+n_i(x,y)<br>$$</p>
<blockquote>
<p>RGB-&gt;Grey</p>
</blockquote>
<p>then,<br>$$<br>D_{i,i+k}(x,y)=|G_i(x,y)-G_{i+k}(x,y)|<br>=|n_i(x,y)-n_{i+k}(x,y)|<br>$$<br><strong><em>Step2:估计噪声分布</em></strong></p>
<p>根据直方图，heavy tail对应outlier，是target；拟合方法采用指数分布<br>$$<br>c_E(x;\lambda)=\begin{cases}<br>1-exp(-\lambda x), &amp;\ x&gt;0 \<br>0,&amp;x \leq \mbox{0}\end{cases}<br>$$<br><strong><em>Step3:<strong>**</strong>二值化</em></strong></p>
<p>根据[7]，引入一个预定义的概率值$p_{fa}$来推导阈值<br>$$<br>th=c_E^{-1}(1-p_{fa};\lambda)<br>$$<br>-1代表逆函数，这里设置参数$p_{fa}$为0.05，大于阈值th被认为是outlier</p>
<p><strong><em>Step4：逻辑与</em></strong></p>
<p>In addition to eliminating ambiguities, logical AND<br>also reduces the existing noises due to their random appearing.</p>
<h3 id="Region-Growing-and-Multi-Morphological-Cue-Based-Discrimination"><a href="#Region-Growing-and-Multi-Morphological-Cue-Based-Discrimination" class="headerlink" title="Region Growing and Multi-Morphological-Cue Based Discrimination"></a>Region Growing and Multi-Morphological-Cue Based Discrimination</h3><p>仍然可能有噪声，包括不规则噪声和规则噪声，不规则噪声是由剧烈光照变化或轻微偏差引起的，随机出现在一些连续帧中，<strong>可以被KF逐步消除</strong></p>
<p>另一种是规则噪声，背景均匀移动，他是与移动目标混淆的重要一点。</p>
<p>用一个术语<em>pseudo motion</em>代称</p>
<p>算法：</p>
<p>1.车辆是2D时域中的一个奇异点，相反的，这些规则噪声的邻域内相似分布，如果<em>candidates</em>可以被链接到相似的邻域像素，那就可以根据形状区分，车辆是矩形，噪声是奇怪形状</p>
<p>邻域被定义为11*11</p>
<p>相似度度量使用高斯分布</p>
<p>被确定连通的像素的阈值应该是<br>$$<br>th_G^-=c_G^-(p_{fa}^-\mu,\sigma)\<br>th_G^+=c_G^{-1}(p_{fa}^+\mu,\sigma)<br>$$<br>两个参数选定为0.005和1-0.005</p>
<p>2.区域生长后，采用一系列形态学特征来区分车辆目标和噪声。</p>
<p>这些特征包括（candidate直接叫做候选了）</p>
<p><em>Area</em>：候选的面积</p>
<p><em>Extent</em>：候选的像素与候选框像素比</p>
<p><em>Major Axis Length</em>：如果一个椭圆与候选连接区域的标准化第二中心相同，那么椭圆半长轴定义为主轴长</p>
<p><em>Eccentricity</em>：上述椭圆的离心率</p>
<p>面积和长轴衡量大小，范围和离心率衡量候选与矩形的相似性。</p>
<p>注意，视频的车当然是刚体</p>
<h2 id="Metric"><a href="#Metric" class="headerlink" title="Metric"></a>Metric</h2><p><strong><em>Precision</em></strong><br>$$<br>P=\frac{TP}{TP+FP}<br>$$<br>TP：检测出的，并且是目标</p>
<p>FP：检测出的，不是目标</p>
<p>FN：没检测出的目标</p>
<p><strong><em>Recall</em></strong><br>$$<br>R=\frac{TP}{TP+FN}<br>$$<br><strong><em>F1-score</em></strong><br>$$<br>F_1=2\frac{P*R}{P+R}<br>$$</p>
<hr>
<p> <strong><em>Jaccard similarity</em></strong><br>$$<br>J=\frac{TP}{TP+FP+FN}<br>$$<br><strong><em>MOTA</em></strong><br>$$<br>MOTA=1-\frac{\sum_i(FN_i+FP_i+IDSW_i)}{\sum_iGT_i}<br>$$<br>IDSW：ID switch</p>
<p>指标范围从负无穷到1，越大越好</p>
<p><strong><em>MOTP</em></strong><br>$$<br>MOTP=\frac{\sum_iIoU_i}{\sum_iM_i}<br>$$<br>从0到1</p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3><h4 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h4><h4 id="Competitor："><a href="#Competitor：" class="headerlink" title="Competitor："></a>Competitor：</h4><p>GMM、ViBe、HCF，+KF</p>
<h4 id="GT"><a href="#GT" class="headerlink" title="GT"></a>GT</h4><p>考虑到工作量，标注了随机抽取的三个500*500区域</p>
<p>每10帧标记一下</p>
<p>其他帧的情况通过线性插值得到</p>
<p>使用Ground Truth Labeler App inMATLAB 2018a </p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>移动目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>On Learning Vehicle Detection in Satellite Video</title>
    <url>/On-Learning-Vehicle-Detection-in-Satellite-Video.html</url>
    <content><![CDATA[<p>通过一个简单网络提取特征并转化为热力图，然后进行阈值分割和最终预测。</p>
<a id="more"></a>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>航空和卫星图像的分辨率和数据量都很大，例如广域运动图像WAMI提供每帧400MP和每秒三帧。卫星图像30FPS的4K RGB图像，并且可能覆盖几十平方公里，每张图中有上千个小物体并且上百个种类。根据GSD，卫星图像中的目标可能只有10px量级。</p>
<p>目标检测变得非常模糊，对噪声和干扰非常敏感，搜索空间急剧增加，变得非常稀疏。人甚至都看不出来物体，这也造成数据标注的大问题。因此，目前研究主要依靠背景差分和帧差法。 [16, 28, 32, 18, 4, 3]</p>
<p>深度学习的方法都是用不同场景的狭窄数据集进行测试的，这也使得结果有效性受到质疑。</p>
<p>因此本文研究了卫星图像中的车辆检测问题，SkySat-1卫星提供120s，30Hz，2K全色视频，面积$1.1km^2$，GSD80cm。Jilin-1提供4MP像素的彩色视频。</p>
<p>这是第一次使用神经网络和深度学习来直接回归卫星视频中车辆位置。受最近在WAMI上工作的启发，文章提出一种开发时间一致性的深度学习方法。为了克服数据问题，将WAMI的工作迁移学习到卫星视频中。</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>在滑窗顶部使用分类器是一种可能的路径。但是还没有，已有的航空图像方法拿来用都不好。</p>
<p>另一个想法是对车辆和背景进行像素级分类（语义分割）[23]将Inception和Resnet结合给出一个热力图，然后选取固定车辆大小和非极大值抑制得到结果。</p>
<p>如何使用时空信息是一个重要问题，The standard is to use background subtraction (BGS) [35, 16, 28, 32, 1] and frame differencing(FD)[18,4,3]。另外，[2]提出将YOLO和时空滤波器结合，[22]提出将KLT跟踪用在视频上，with a SegNet on overlapping multispectral data。[35]使用CIFAR训练的resnet分类器去处理混合高斯前景模型的结果。这里的标准是应用连接成分分析[16，28]，显著性分析，分割[32，18]，分布拟合[4，3]，然后是形态学。</p>
<p>还有一个大问题是稀疏性，如在WAMI中，通过将大图像进行聚类再去做。</p>
<p>最近，[10，9，26，12]也考虑了飞机、火车和车辆的跟踪，或者使用光流[10，9]，相关跟踪器（KLT）[26]或相关和卡尔曼滤波器的组合[12]。</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>受[17]启发，他设计了两个神经网络，称为ClusterNet和FoveaNet，检测WAMI中的移动目标。ClusterNet提出region of object(ROOBI)，基于areas of interest(AOI)，作为FoveaNet的输入。文章没有使用ClusterNet，而是直接将AOI裁剪成128*128的ROOBI。</p>
<h3 id="3-1-FoveaNet-and-thresholding"><a href="#3-1-FoveaNet-and-thresholding" class="headerlink" title="3.1 FoveaNet and thresholding"></a>3.1 FoveaNet and thresholding</h3><p><img src="/On-Learning-Vehicle-Detection-in-Satellite-Video/image-20201121162541315.png" alt="image-20201121162541315"></p>
<p>这个网络由全卷积层构成，每层滤波器的数量分别是32,32,32,256,512,256,256 and 1。</p>
<p>第一层后接一个2*2池化层，第6,7层加了0.5dropout，最后一层每个神经元在像素级对移动车辆可能性投票。</p>
<p>输入网络的是一个$N\times N\times c$的帧栈，N是ROOBI size，c是连续帧。CNN应该学到检测中心帧的目标位置。文章举例c=5。</p>
<p>地面真实情况基于热图H，该热图是通过叠加高斯分布创建的，其中每个分布的中心是图像中车辆的像素位置，其中n是像素位置提供的下采样地面真实坐标，σ是高斯模糊的方差。在训练过程中，网络学习最小化网络输出和生成的地面真实热图之间的欧几里德距离</p>
<p>第二步处理预测的热图以确定物体的位置。为此，通过OTSU阈值化将热图转换为分割图[17]。如果分割区域大于阈值α，则区域中心被定义为对象位置。</p>
<h3 id="3-2-迁移学习"><a href="#3-2-迁移学习" class="headerlink" title="3.2 迁移学习"></a>3.2 迁移学习</h3><p>the WPAFB images have about four times higher GSD than the LasVegas video，为此，我们根据WPAFB数据集训练CNN。之后我们对CNN卫星视频数据进行微调。</p>
<h2 id="Experimental-Evaluation-and-Results"><a href="#Experimental-Evaluation-and-Results" class="headerlink" title="Experimental Evaluation and Results"></a>Experimental Evaluation and Results</h2><table>
<thead>
<tr>
<th>参数</th>
<th>值</th>
</tr>
</thead>
<tbody><tr>
<td>batch size</td>
<td>32</td>
</tr>
<tr>
<td>lr</td>
<td>1e-5</td>
</tr>
</tbody></table>
<p>数据准备包括帧注册和相机运动补偿。</p>
<p>三组实验，第一组复现17的结果，第二组降低WPAFB的GSD，这样就和卫星图像相似，第三组，微调并评估结果。</p>
<p>标准：检测结果在gt的一定距离$\theta$内就认为TP，如果范围内有多个结果，最近的当做TP，剩下的如果没有被确定为其他TP就是FP，没在范围内的自然也是FP，范围内一个检测结果也没有的叫做FN。结果将按照P,R,F1比较。</p>
<p>选取与17一样的AOI，编号为34,40,41。AOI 40在一个主要交叉口处有大量密集交通，而AOI 41主要由道路上的单车组成。AOI34是两种交通模式的组合。训练就选其中两个训练另一个。</p>
<p>如果目标在5帧内移动$\omega$像素，记为移动目标。</p>
<h3 id="4-1-Experiment-1-Baseline-evaluation"><a href="#4-1-Experiment-1-Baseline-evaluation" class="headerlink" title="4.1 Experiment 1: Baseline evaluation"></a>4.1 Experiment 1: Baseline evaluation</h3><p>复现17结果。</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>值</th>
</tr>
</thead>
<tbody><tr>
<td>N(ROOBI edge length)</td>
<td>100px</td>
</tr>
<tr>
<td>$\sigma$(variance of Gaussian blur)</td>
<td>2</td>
</tr>
<tr>
<td>$\theta$(evaluation threshold)</td>
<td>40px</td>
</tr>
<tr>
<td>$\omega$(threshold for removing stationary cars)</td>
<td>15px</td>
</tr>
<tr>
<td>$\alpha$(threshold to disregard small segments)</td>
<td>15px</td>
</tr>
</tbody></table>
<h3 id="4-2-Experiment-2-调整GSD"><a href="#4-2-Experiment-2-调整GSD" class="headerlink" title="4.2 Experiment 2: 调整GSD"></a>4.2 Experiment 2: 调整GSD</h3><p>在第二个实验中，我们用0.4和0.2的比例因子（SF）来降低图像，结果分别是原始图像分辨率的40%和20%。我们选择的SF为0.2，因为这个因子将WPAFB数据集中的典型车辆对象大小从18×9px减小到3.6×1.8px，这与卫星视频中的车辆大小类似。实验设置了以下参数：SF=0.4，N=100px，σ=2，θ=16px，ω=6px，α=15px，SF=0.2，N=100px，σ=1，θ=8px，ω=3px，α=3.5px</p>
<p>结果表明，随着GSD变大，效果显著变差，但是c变大会有效提高分数。</p>
<p>另外，结果有严重的黏连问题，文章没有很好的处理，只是妥协性的降低了$\sigma$。</p>
<h3 id="4-3-Experiment-3-Satellite-video"><a href="#4-3-Experiment-3-Satellite-video" class="headerlink" title="4.3 Experiment 3: Satellite video"></a>4.3 Experiment 3: Satellite video</h3><p>对于训练和评估，我们设置θ=8px，α=4px，σ=1，c=5和N=128px。另外，我们设置SF=0.2和ω=3px来训练WPAFB数据集。本实验观察到用leaky ReLUs代替ELUs，提高了训练效率。</p>
<p>[17]关于空间上下文信息需要在第一层中使用大过滤器的观点似乎是误导性的，因为上下文是由网络的接受场引入深层网络的高层的。我们认为，滤波器的大小取决于车辆在连续帧中的像素距离，这样时空网络就可以利用时间信息，这是我们实验所证实的</p>
<p>结果还表明，学习检测移动目标通过线性运动的斜率比学习视觉外观的时空变化简单。预训练对泛化非常重要。</p>
<p>最后，关于帧率的实验表明高帧率对结果没有大影响，因为需要学习的特征很简单。</p>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>深度学习方法</category>
      </categories>
      <tags>
        <tag>移动目标检测</tag>
      </tags>
  </entry>
</search>
